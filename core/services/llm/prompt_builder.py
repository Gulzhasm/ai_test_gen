#!/usr/bin/env python3
"""
Dynamic Prompt Builder for Project-Agnostic Test Generation

This module is AI TEST-GEN  that:
1. Reads and understands project YAML configuration completely
2. Extracts all relevant information from user stories and acceptance criteria
3. Builds comprehensive LLM prompts that instruct the model to act as an
   expert QA engineer with 10+ years of experience

The prompts generated by this module guide LLMs to:
- Understand the application under test from project configuration
- Parse and comprehend acceptance criteria semantically
- Generate/correct sensible, human-quality test cases
- Apply context-aware testing (no boundary tests for Help menus, etc.)

KEY FEATURES:
- Full project configuration understanding (from YAML)
- Deep acceptance criteria parsing and semantic understanding
- Context-aware test generation based on feature type detection
- Platform-specific accessibility tests based on supported platforms
- Humanistic, sensible test generation that mimics expert QA engineers
"""
from typing import List, Dict, Optional, Any, Tuple
from dataclasses import dataclass
import re


# Feature type detection patterns
# Note: Use partial word boundaries where appropriate to match verb conjugations (e.g., enter/enters/entered)
FEATURE_TYPE_PATTERNS = {
    'input': [
        r'\binput', r'\benter', r'\btype\b', r'\bfield\b', r'\bform\b',
        r'\btextbox', r'\bdropdown', r'\bselect\b.*\bvalue\b', r'\bset\b.*\bvalue\b',
        r'\bedit\b.*\bvalue\b', r'\bchange\b.*\bvalue\b', r'\bmodify\b.*\bvalue\b',
        r'\bcoordinat', r'\blatitud', r'\blongitud', r'\btext\s+(?:box|field)',
        r'\bfill\s+in\b', r'\bprovid', r'\bspecif'  # provide, provides, specify, specifies
    ],
    'calculation': [
        r'\bcalculat', r'\bcomput', r'\bsum\b', r'\btotal', r'\bformula',
        r'\bconvert', r'\bmeasur', r'\bdimension', r'\bvalue\b.*\bdisplay',
        r'\bresult', r'\bmath', r'\barithmetic'
    ],
    'navigation': [
        r'\bmenu\b', r'\bnavigate', r'\bopen\b', r'\bclose\b', r'\baccess',
        r'\bhelp\b', r'\babout\b', r'\bmanual\b', r'\bguide\b', r'\bdocument',
        r'\bfile\s+menu', r'\bedit\s+menu', r'\btools?\s+menu', r'\bview\s+menu'
    ],
    'display': [
        r'\bview(?:er)?\b', r'\bdisplay', r'\bshow', r'\bappear', r'\bvisible',
        r'\bread\b', r'\bpdf\b', r'\bimage\b', r'\breport', r'\bpreview',
        r'\bread-only', r'\bsee\b', r'\bwatch'
    ],
    'object_manipulation': [
        r'\brotat', r'\bmov(?:e|es|ed|ing)\b', r'\bresiz', r'\bscal', r'\bflip',
        r'\bmirror', r'\bduplic', r'\bcopy', r'\bpaste', r'\bdelet',
        r'\bdraw', r'\bcreate\b.*\bobject', r'\bshape\b', r'\btool\b',
        r'\btransform', r'\breposition', r'\bselect\b.*\bobject'
    ]
}


def detect_feature_type(feature_name: str, acceptance_criteria: List[str]) -> str:
    """
    Detect the type of feature being tested to generate appropriate test types.

    Returns one of: 'input', 'calculation', 'navigation', 'display', 'object_manipulation', 'general'
    """
    combined_text = f"{feature_name} {' '.join(acceptance_criteria)}".lower()

    scores = {}
    for feature_type, patterns in FEATURE_TYPE_PATTERNS.items():
        score = sum(1 for pattern in patterns if re.search(pattern, combined_text, re.IGNORECASE))
        scores[feature_type] = score

    # Return the type with highest score, or 'general' if no matches
    if max(scores.values()) == 0:
        return 'general'

    return max(scores, key=scores.get)


@dataclass
class PromptContext:
    """Context for building dynamic prompts."""
    app_name: str
    app_type: str  # desktop, web, mobile, hybrid
    story_id: str
    feature_name: str
    acceptance_criteria: List[str]
    qa_prep: str

    # Application constraints
    unavailable_features: List[str]
    feature_notes: Dict[str, str]

    # UI configuration
    ui_surfaces: List[str]
    entry_points: Dict[str, str]

    # Platform support (e.g., ['Windows 11', 'iPad', 'Android Tablet'])
    platforms: List[str]

    # Step templates
    prereq_template: str
    launch_step: str
    launch_expected: str
    close_step: str

    # Test rules
    forbidden_words: List[str]
    allowed_areas: List[str]

    @property
    def feature_type(self) -> str:
        """Detect the type of feature for context-aware test generation."""
        return detect_feature_type(self.feature_name, self.acceptance_criteria)


class PromptBuilder:
    """
    Builds dynamic, project-agnostic prompts for LLM-based test generation.

    This class acts as an expert AI QA Automation Engineer, creating prompts
    that guide LLMs to generate comprehensive, human-quality test cases
    adaptable to any application.
    """

    def __init__(self, context: PromptContext):
        self.ctx = context

    @classmethod
    def from_project_config(
        cls,
        config,  # ProjectConfig
        story_id: str,
        feature_name: str,
        acceptance_criteria: List[str],
        qa_prep: str = ""
    ) -> 'PromptBuilder':
        """Create PromptBuilder from a ProjectConfig."""
        context = PromptContext(
            app_name=config.application.name,
            app_type=config.application.app_type,
            story_id=story_id,
            feature_name=feature_name,
            acceptance_criteria=acceptance_criteria,
            qa_prep=qa_prep,
            unavailable_features=getattr(config.application, 'unavailable_features', []),
            feature_notes=getattr(config.application, 'feature_notes', {}),
            ui_surfaces=config.application.main_ui_surfaces,
            entry_points=config.application.entry_point_mappings,
            platforms=config.application.supported_platforms,
            prereq_template=config.application.prereq_template,
            launch_step=config.application.launch_step,
            launch_expected=config.application.launch_expected or "",
            close_step=config.application.close_step,
            forbidden_words=config.rules.forbidden_words,
            allowed_areas=config.rules.allowed_areas,
        )
        return cls(context)

    def build_system_prompt(self) -> str:
        """
        Build the system prompt dynamically based on project configuration.

        This creates a comprehensive prompt that trains the LLM to act as
        an expert QA engineer for the specific application.

        IMPORTANT: Uses feature type detection to generate CONTEXT-APPROPRIATE tests.
        - Navigation/Display features (Help, About, View) -> NO boundary/input tests
        - Input features (Forms, Text fields) -> Include boundary/input tests
        - Object manipulation features (Draw, Edit) -> Include state/undo tests
        """
        # Build application constraints section
        constraints_section = self._build_constraints_section()

        # Build UI surfaces section
        ui_section = self._build_ui_section()

        # Build step templates section
        step_templates_section = self._build_step_templates_section()

        # Build platform section
        platform_section = self._build_platform_section()

        # Build context-appropriate test coverage section
        test_coverage_section = self._build_context_aware_test_coverage()

        # Build expert persona and project understanding
        expert_persona = self._build_expert_persona()
        project_understanding = self._build_project_understanding()
        ac_understanding = self._build_ac_understanding()

        return f'''{expert_persona}

{project_understanding}

{ac_understanding}

## FEATURE BEING TESTED
- **Feature Name**: {self.ctx.feature_name}
- **Feature Type Detected**: {self.ctx.feature_type}
- **Story ID**: {self.ctx.story_id}
{constraints_section}

## CRITICAL: CONTEXT-APPROPRIATE TESTING
You MUST generate tests that are RELEVANT to the feature type:
- For NAVIGATION/DISPLAY features (Help, About, View menus): Focus on accessibility, visibility, content display
- For INPUT features (Forms, Text fields): Include input validation, boundary values
- For OBJECT MANIPULATION features (Draw, Edit tools): Include state changes, undo/redo

DO NOT add irrelevant tests! A Help menu does NOT need:
- Input validation tests (there's no input field!)
- Boundary value tests (there are no numeric values!)
- Maximum/minimum value tests (nonsensical for menus!)

## YOUR EXPERT QA MINDSET
Think like a human QA expert with 10+ years of experience:
- "What tests actually make sense for THIS specific feature?"
- "What are the REAL risks and edge cases for '{self.ctx.feature_name}'?"
- "What could go wrong that a developer might miss?"
- "What are realistic user scenarios that matter?"
- "What would I test if I had limited time but needed full coverage?"

## YOUR RESPONSIBILITIES AS AN EXPERT QA ENGINEER
1. **UNDERSTAND** the feature deeply from acceptance criteria and QA prep
2. **CORRECT** existing test cases (formatting, language, structure)
3. **REMOVE** any tests that don't make sense for this feature type
4. **ADD** only RELEVANT edge case tests based on feature type
5. **ADD** accessibility tests for ALL supported platforms
6. **WRITE** tests as a seasoned QA professional would write them
7. **THINK** about what a real user would actually do with this feature

{ui_section}

## TITLE RULES (CRITICAL)
Format: "<StoryID>-<ID>: <Feature> / <Area> / <Scenario>"

NOTE: Do NOT add platform suffixes to regular test titles.
Only accessibility tests should specify platform.

ALLOWED Areas:
{self._format_list(self.ctx.allowed_areas or self.ctx.ui_surfaces)}

FORBIDDEN Areas (never use):
- Functionality, Behavior (alone), Validation, General, System

{step_templates_section}

## EXPECTED RESULT RULES (CRITICAL FOR QUALITY)
Steps that MUST have EMPTY expected:
- PRE-REQ steps (prerequisite conditions)
- Close/Exit the application

Steps that MUST have MEANINGFUL expected results:
- Launch/Navigate steps (e.g., "Application opens", "Menu displays")
- Verification steps ("Verify...", "Check...", "Confirm...")
- Action steps that change state (e.g., "Object is created", "Selection is visible")
- Menu/dialog actions (e.g., "Dialog appears", "Options are displayed")

QUALITY RULE: At least 50% of steps should have expected results.
If a step causes something VISIBLE to happen, it NEEDS an expected result.
Example GOOD steps:
  Action: "Click the Save button" → Expected: "File is saved and confirmation appears"
  Action: "Select the ellipse" → Expected: "Ellipse is highlighted with selection handles"
  Action: "Open the Dimensions Menu" → Expected: "Dimensions Menu opens with available commands"

## FORBIDDEN LANGUAGE (HARD FAIL)
Remove: {', '.join(self.ctx.forbidden_words[:5])}
Also remove: "if available", "if supported", "e.g., X or Y"

## FORBIDDEN PLATFORM TERMS (NEVER USE)
- NEVER use "Mobile" or "Tablet/Mobile" - there is NO mobile app
- NEVER use generic terms like "tablet" alone
- ALWAYS use exact platform names: {', '.join(self.ctx.platforms)}
- For touch tests, specify the exact platform (e.g., "iPad" or "Android Tablet")

## ID SEQUENCE
- AC1 for first test (availability)
- Then increment by 5: 005, 010, 015, 020...

{test_coverage_section}

{platform_section}

## OBJECTIVE FIELD FORMAT
Each test case MUST have an "objective" field that:
- Starts with "Verify that"
- Is specific to the actual feature being tested
- Example: "Verify that {self.ctx.feature_name} opens correctly"

## FINAL CHECKLIST:
1. All tests are RELEVANT to "{self.ctx.feature_name}" feature type ({self.ctx.feature_type})
2. NO irrelevant tests (no boundary tests for menus, no input tests for viewers)
3. MANDATORY: {len(self.ctx.platforms)} SEPARATE accessibility tests for: {', '.join(self.ctx.platforms)}
   - Each test title MUST include the exact platform name in parentheses
   - Each test MUST have platform-specific pre-requisites and steps
4. All tests have proper objective field
5. No forbidden language used
6. ID sequence correct (AC1, then 005, 010, 015...)
7. Include edge case, negative, and state transition tests
8. Expected total: 15-25 comprehensive test cases

Output corrected JSON only: {{"test_cases": [...]}}'''

    def build_user_prompt(self, test_cases_json: str) -> str:
        """
        Build the user prompt dynamically based on project configuration.
        Uses feature type detection for context-appropriate test suggestions.
        """
        # Format acceptance criteria
        ac_text = "\n".join([f"{i+1}. {ac}" for i, ac in enumerate(self.ctx.acceptance_criteria)])

        # Build constraints warning
        constraints_warning = self._build_constraints_warning()

        # Build context-appropriate tasks
        context_tasks = self._build_context_aware_user_tasks()

        # Build sensible test generation guidance
        sensible_test_guidance = self._build_sensible_test_guidance()

        return f'''You are an EXPERT QA ENGINEER with 10+ years of experience in software testing.
Your task: Review, correct, and enhance test cases for Story {self.ctx.story_id}: {self.ctx.feature_name}

## YOUR MISSION
As an expert QA engineer, you will:
1. **READ** the project configuration to understand the application
2. **ANALYZE** the acceptance criteria to identify testable requirements
3. **REVIEW** existing test cases for quality and completeness
4. **GENERATE** sensible, human-quality test cases that provide real value

## APPLICATION CONTEXT (from Project Config)
- **Application**: {self.ctx.app_name}
- **Type**: {self.ctx.app_type}
- **Platforms**: {', '.join(self.ctx.platforms)}
- **Feature**: {self.ctx.feature_name}
- **Feature Type Detected**: {self.ctx.feature_type}
{constraints_warning}

## ACCEPTANCE CRITERIA (Your Primary Testing Source)
Carefully read each criterion - these define WHAT must be tested:
{ac_text}

## QA PREP SUMMARY (Additional Testing Context)
{self.ctx.qa_prep[:2000] if self.ctx.qa_prep else "Not provided - use your expert judgment based on the acceptance criteria above"}

## FEATURE TYPE GUIDANCE
This is a **{self.ctx.feature_type}** feature.
{self._get_feature_type_guidance()}

{sensible_test_guidance}

## CURRENT TEST CASES TO REVIEW:
{test_cases_json}

## YOUR TASKS AS AN EXPERT QA ENGINEER:

### TASK 1: UNDERSTAND THE FEATURE
- What is the core functionality being delivered?
- What are the key user workflows?
- What could go wrong in real usage?

### TASK 2: FIX AND CLEAN EXISTING TESTS
- Remove tests that don't make sense for this feature type
- Fix forbidden language ("or", "if available", "if supported")
- Fix title format: "<StoryID>-<ID>: <Feature> / <Area> / <Scenario>"
- Ensure PRE-REQ first, Close/Exit last
- **ADD MISSING EXPECTED RESULTS** to action steps:
  - "Create object" → "Object appears on canvas"
  - "Select item" → "Item is highlighted/selected"
  - "Open menu" → "Menu displays with options"
  - "Click button" → "Action is performed and feedback shown"

{context_tasks}

### TASK 3: ENSURE ACCESSIBILITY TESTS
{self._build_accessibility_task()}

### TASK 4: ADD SENSIBLE TEST COVERAGE
Think: "What tests would I write if I had limited time but needed thorough coverage?"
- Edge cases (empty state, no selection, rapid actions)
- Negative tests (invalid inputs, wrong conditions)
- State tests (undo/redo if applicable)
- Touch tests for: {', '.join([p for p in self.ctx.platforms if 'windows' not in p.lower()])}

## STEP TEMPLATES
- Pre-requisite: "{self.ctx.prereq_template.format(app_name=self.ctx.app_name)}"
- Launch: "{self.ctx.launch_step.format(app_name=self.ctx.app_name)}"
- Close: "{self.ctx.close_step.format(app_name=self.ctx.app_name)}"

## OUTPUT
Return JSON with high-quality test cases: {{"test_cases": [...]}}

Target: very well-crafted test cases, test case count depending on feature complexity, typically 15-25 tests including:
- Core functionality tests (map to AC items)
- {len(self.ctx.platforms)} accessibility tests (one per platform)
- Relevant edge cases and negative tests
- State transition tests (where applicable)

REMEMBER: Quality over quantity. Every test should answer: "What bug would this catch?"'''

    def _build_expert_persona(self) -> str:
        """Build the expert QA engineer persona section."""
        return f'''You are an EXPERT QA ENGINEER with 10+ years of experience in software testing.

## YOUR EXPERTISE
- You have tested hundreds of {self.ctx.app_type} applications across multiple domains
- You understand the difference between valuable tests and checkbox testing
- You write tests that FIND REAL BUGS, not just cover requirements superficially
- You think about edge cases that developers forget
- You know when a test is unnecessary or redundant
- You write clear, actionable test steps that any tester can follow

## YOUR APPROACH
1. **Read and understand** the project configuration to know the application deeply
2. **Analyze acceptance criteria** to extract testable requirements
3. **Think critically** about what could go wrong in real usage
4. **Generate sensible tests** that provide actual value, not just coverage metrics
5. **Write like a human** - your tests should read naturally, not like a robot wrote them'''

    def _build_project_understanding(self) -> str:
        """Build the project configuration understanding section."""
        lines = [f'''## APPLICATION UNDER TEST (from Project Configuration)

**Application Profile:**
- **Name**: {self.ctx.app_name}
- **Type**: {self.ctx.app_type}
- **Supported Platforms**: {', '.join(self.ctx.platforms) if self.ctx.platforms else 'Not specified'}''']

        # Add UI surfaces understanding
        if self.ctx.ui_surfaces:
            lines.append("\n**Available UI Surfaces** (use these for test areas):")
            for surface in self.ctx.ui_surfaces[:8]:  # Top 8 surfaces
                lines.append(f"  - {surface}")
            if len(self.ctx.ui_surfaces) > 8:
                lines.append(f"  - ... and {len(self.ctx.ui_surfaces) - 8} more")

        # Add entry points understanding
        if self.ctx.entry_points:
            lines.append("\n**Feature Entry Points** (how users access features):")
            for keyword, entry_point in list(self.ctx.entry_points.items())[:6]:
                lines.append(f"  - '{keyword}' features → {entry_point}")

        # Add step templates
        lines.append("\n**Standard Test Step Structure:**")
        lines.append(f"  - Pre-req: \"{self.ctx.prereq_template.format(app_name=self.ctx.app_name)}\"")
        lines.append(f"  - Launch: \"{self.ctx.launch_step.format(app_name=self.ctx.app_name)}\"")
        if self.ctx.launch_expected:
            lines.append(f"  - Launch Expected: \"{self.ctx.launch_expected}\"")
        lines.append(f"  - Close: \"{self.ctx.close_step.format(app_name=self.ctx.app_name)}\"")

        return "\n".join(lines)

    def _build_ac_understanding(self) -> str:
        """Build the acceptance criteria understanding section."""
        if not self.ctx.acceptance_criteria:
            return ""

        lines = ['''## UNDERSTANDING ACCEPTANCE CRITERIA

As an expert QA engineer, you must:
1. **Parse each AC item** to identify the specific testable requirement
2. **Identify the action** - what the user does
3. **Identify the expected outcome** - what should happen
4. **Consider variations** - what are the edge cases for this AC?
5. **Map to test cases** - create focused tests that verify each AC

**Key Questions to Ask for Each AC:**
- What is the MINIMUM test needed to verify this works?
- What could cause this to FAIL in production?
- Are there BOUNDARY conditions to consider?
- What happens if the user does something UNEXPECTED?
- Is there UNDO/REDO behavior to verify?''']

        # Add parsed AC summary
        lines.append(f"\n**Acceptance Criteria Summary** ({len(self.ctx.acceptance_criteria)} items):")
        for i, ac in enumerate(self.ctx.acceptance_criteria[:5], 1):
            ac_preview = ac[:100] + "..." if len(ac) > 100 else ac
            lines.append(f"  {i}. {ac_preview}")
        if len(self.ctx.acceptance_criteria) > 5:
            lines.append(f"  ... and {len(self.ctx.acceptance_criteria) - 5} more")

        return "\n".join(lines)

    def _build_sensible_test_guidance(self) -> str:
        """Build guidance for generating sensible, human-quality tests."""
        feature_type = self.ctx.feature_type

        lines = ['''## SENSIBLE TEST GENERATION PRINCIPLES

As an expert QA engineer with 10+ years of experience, you know that:

**Good Tests:**
- Have a clear PURPOSE (what bug would this catch?)
- Are SPECIFIC to the feature being tested
- Cover realistic USER SCENARIOS
- Include meaningful EXPECTED RESULTS
- Are MAINTAINABLE and easy to understand

**Bad Tests (AVOID):**
- Generic tests copied from templates
- Tests that don't match the feature type
- Redundant tests that verify the same thing
- Tests with vague or missing expected results
- Tests that can never fail (useless)''']

        # Add feature-specific guidance
        if feature_type == 'navigation':
            lines.append('''
**For Navigation Features:**
- Focus on: menu access, navigation paths, keyboard shortcuts
- DON'T add: input validation, boundary values (no inputs to validate!)''')
        elif feature_type == 'display':
            lines.append('''
**For Display Features:**
- Focus on: content visibility, formatting, scroll/zoom behavior
- DON'T add: input tests (it's read-only!), numeric boundaries''')
        elif feature_type == 'input':
            lines.append('''
**For Input Features:**
- DO include: validation, boundary values, error handling
- Test: empty, minimum, maximum, invalid, special characters''')
        elif feature_type == 'object_manipulation':
            lines.append('''
**For Object Manipulation Features:**
- Focus on: single object ops, undo/redo, state persistence
- DON'T add: multi-select tests (if not supported)''')

        return "\n".join(lines)

    def _build_constraints_section(self) -> str:
        """Build the application constraints section."""
        if not self.ctx.unavailable_features and not self.ctx.feature_notes:
            return ""

        lines = ["\n## CRITICAL APPLICATION CONSTRAINTS"]

        if self.ctx.unavailable_features:
            lines.append(f"Features NOT available in {self.ctx.app_name}:")
            for feature in self.ctx.unavailable_features:
                lines.append(f"  - {feature}")
            lines.append("")
            lines.append("NEVER generate tests for these unavailable features!")
            lines.append("If you see tests for unavailable features, REMOVE them.")

        if self.ctx.feature_notes:
            lines.append(f"\nFeature notes/limitations for {self.ctx.app_name}:")
            for feature, note in self.ctx.feature_notes.items():
                lines.append(f"  - {feature}: {note}")

        return "\n".join(lines)

    def _build_constraints_warning(self) -> str:
        """Build constraints warning for user prompt."""
        if not self.ctx.unavailable_features:
            return ""

        lines = [f"## CRITICAL CONSTRAINTS FOR {self.ctx.app_name.upper()}"]
        for feature in self.ctx.unavailable_features:
            lines.append(f"- {self.ctx.app_name} does NOT support: {feature}")
        lines.append(f"- REMOVE any tests that reference unavailable features")
        lines.append(f"- Replace with valid edge cases specific to {self.ctx.app_name}")

        return "\n".join(lines)

    def _build_ui_section(self) -> str:
        """Build the UI surfaces section."""
        if not self.ctx.ui_surfaces:
            return ""

        lines = [f"\n## UI SURFACES IN {self.ctx.app_name.upper()}"]
        lines.append("Available UI surfaces for test scenarios:")
        for surface in self.ctx.ui_surfaces:
            lines.append(f"  - {surface}")

        if self.ctx.entry_points:
            lines.append("\nFeature entry points:")
            for keyword, entry_point in list(self.ctx.entry_points.items())[:10]:
                lines.append(f"  - {keyword} -> {entry_point}")

        return "\n".join(lines)

    def _build_step_templates_section(self) -> str:
        """Build the step templates section."""
        prereq = self.ctx.prereq_template.format(app_name=self.ctx.app_name)
        launch = self.ctx.launch_step.format(app_name=self.ctx.app_name)
        close = self.ctx.close_step.format(app_name=self.ctx.app_name)

        lines = [
            f"\n## STEP STRUCTURE FOR {self.ctx.app_name.upper()}",
            f"1. First step: Pre-requisite step (empty expected)",
            f'   Action: "{prereq}"',
            f"2. Second step: Launch/Navigate step",
            f'   Action: "{launch}"',
        ]

        if self.ctx.launch_expected:
            lines.append(f'   Expected: "{self.ctx.launch_expected}"')

        lines.extend([
            f"3. Last step: Close/Exit step (empty expected)",
            f'   Action: "{close}"',
        ])

        return "\n".join(lines)

    def _build_platform_section(self) -> str:
        """Build the platform/accessibility section."""
        if not self.ctx.platforms:
            return ""

        lines = ["\n## MANDATORY ACCESSIBILITY TESTS"]
        lines.append("CRITICAL: You MUST create a SEPARATE accessibility test case for EACH platform below.")
        lines.append(f"The following {len(self.ctx.platforms)} platforms are supported by {self.ctx.app_name}:")
        lines.append("")

        for i, platform in enumerate(self.ctx.platforms, 1):
            platform_lower = platform.lower()
            if 'windows' in platform_lower:
                lines.append(f"  {i}. **{platform}**: Create test with keyboard navigation + Accessibility Insights for Windows")
                lines.append(f"     - Title must include \"({platform})\"")
                lines.append("     - Pre-req: Accessibility Insights for Windows tool is installed")
            elif 'ipad' in platform_lower or 'ios' in platform_lower:
                lines.append(f"  {i}. **{platform}**: Create test with VoiceOver + swipe gestures (NO keyboard)")
                lines.append(f"     - Title must include \"({platform})\"")
                lines.append(f"     - Pre-req: VoiceOver is enabled on the {platform}")
            elif 'android' in platform_lower:
                lines.append(f"  {i}. **{platform}**: Create test with Accessibility Scanner + touch gestures")
                lines.append(f"     - Title must include \"({platform})\"")
                lines.append("     - Pre-req: Accessibility Scanner for Android is installed")
            elif 'mac' in platform_lower:
                lines.append(f"  {i}. **{platform}**: Create test with VoiceOver + keyboard navigation")
                lines.append(f"     - Title must include \"({platform})\"")
            else:
                lines.append(f"  {i}. **{platform}**: Create platform-appropriate accessibility test")
                lines.append(f"     - Title must include \"({platform})\"")
            lines.append("")

        lines.append(f"⚠️ REQUIRED: Generate exactly {len(self.ctx.platforms)} separate accessibility test cases!")

        return "\n".join(lines)

    def _build_fix_existing_tasks(self) -> str:
        """Build the fix existing tests tasks."""
        lines = []

        if self.ctx.unavailable_features:
            lines.append(f"- Remove any tests for unavailable features in {self.ctx.app_name}")

        lines.extend([
            "- Fix forbidden language (\"or\", \"if available\", etc.)",
            "- Fix title format issues",
            "- **ADD EXPECTED RESULTS** to ALL action steps that cause visible changes",
            "- Ensure PRE-REQ first, Close/Exit last (both empty expected)",
            "- Target: At least 50% of steps should have expected results",
        ])

        return "\n".join(lines)

    def _build_accessibility_task(self) -> str:
        """Build the accessibility tests task."""
        if not self.ctx.platforms:
            return "Add accessibility tests appropriate for the application type."

        lines = [f"⚠️ MANDATORY: Create {len(self.ctx.platforms)} SEPARATE accessibility tests:"]
        for i, platform in enumerate(self.ctx.platforms, 1):
            platform_lower = platform.lower()
            if 'windows' in platform_lower:
                lines.append(f"  {i}. {platform}: keyboard navigation + Accessibility Insights for Windows")
            elif 'ipad' in platform_lower or 'ios' in platform_lower:
                lines.append(f"  {i}. {platform}: VoiceOver + swipe gestures (NO keyboard)")
            elif 'android' in platform_lower:
                lines.append(f"  {i}. {platform}: Accessibility Scanner + touch gestures")
            elif 'mac' in platform_lower:
                lines.append(f"  {i}. {platform}: VoiceOver + keyboard navigation")
            else:
                lines.append(f"  {i}. {platform}: Platform-appropriate accessibility testing")

        lines.append("")
        lines.append("Each accessibility test MUST:")
        lines.append(f"  - Have the platform name in parentheses in the title, e.g., \"({self.ctx.platforms[0]})\"")
        lines.append("  - Have platform-specific pre-requisites")
        lines.append("  - Use platform-specific testing tools and gestures")

        return "\n".join(lines)

    def _build_context_aware_test_coverage(self) -> str:
        """
        Build test coverage section based on detected feature type.
        Only includes test types that make sense for the feature.
        """
        feature_type = self.ctx.feature_type

        lines = ["## CONTEXT-APPROPRIATE TEST COVERAGE"]
        lines.append(f"For this **{feature_type}** feature, focus on RELEVANT tests only:")

        if feature_type == 'navigation':
            lines.extend([
                "",
                "### NAVIGATION FEATURE TESTS:",
                "- Menu item visibility and accessibility",
                "- Correct navigation path opens expected content",
                "- Back/close behavior returns to previous state",
                "- Keyboard shortcuts work (if applicable)",
                "- Touch navigation works (for tablet platforms)",
                "- Menu state after navigation (enabled/disabled items)",
                "",
                "### DO NOT ADD (irrelevant for navigation):",
                "- Input validation tests (no inputs!)",
                "- Boundary value tests (no numeric values!)",
                "- Maximum/minimum tests (meaningless for menus!)",
            ])

        elif feature_type == 'display':
            lines.extend([
                "",
                "### DISPLAY/VIEWER FEATURE TESTS:",
                "- Content displays correctly",
                "- Content is readable and properly formatted",
                "- Scroll/zoom functionality (if applicable)",
                "- Viewer opens without external dependencies",
                "- Close viewer returns to previous state",
                "- Content persists across app states",
                "",
                "### DO NOT ADD (irrelevant for viewers):",
                "- Input validation tests (read-only content!)",
                "- Numeric boundary tests (no numeric inputs!)",
            ])

        elif feature_type == 'input':
            lines.extend([
                "",
                "### INPUT FEATURE TESTS (boundary tests ARE appropriate):",
                "- Empty input behavior",
                "- Minimum valid value",
                "- Maximum valid value",
                "- Just outside valid range",
                "- Special characters handling",
                "- Very long input handling",
                "- Invalid input type (text vs number)",
                "- Whitespace-only input",
            ])

        elif feature_type == 'calculation':
            lines.extend([
                "",
                "### CALCULATION FEATURE TESTS:",
                "- Correct calculation results",
                "- Edge values (zero, negative, maximum)",
                "- Precision/rounding behavior",
                "- Invalid input handling",
                "- Display format correctness",
            ])

        elif feature_type == 'object_manipulation':
            lines.extend([
                "",
                "### OBJECT MANIPULATION TESTS:",
                "- Single object operations (remember: no multi-select!)",
                "- Undo/Redo functionality",
                "- State persistence after operation",
                "- Operation on different object types",
                "- Operation in different app states",
                "- Rapid repeated operations",
            ])

        else:  # general
            lines.extend([
                "",
                "### GENERAL EDGE CASES:",
                "- Feature availability in expected location",
                "- Feature behavior in different app states",
                "- Undo/redo if applicable",
                "- State persistence",
                "",
                "Only add boundary/input tests if the feature actually has inputs!",
            ])

        return "\n".join(lines)

    def _build_context_aware_user_tasks(self) -> str:
        """Build user tasks based on feature type."""
        feature_type = self.ctx.feature_type

        if feature_type == 'navigation':
            return """### TASK 2: ADD NAVIGATION-APPROPRIATE TESTS
- Menu visibility tests
- Navigation path tests
- Keyboard shortcut tests (if applicable)
- Touch navigation tests (for tablet platforms)
- Close/back behavior tests

### DO NOT ADD:
- Input validation tests (menus don't have inputs!)
- Boundary value tests (menus don't have numeric values!)"""

        elif feature_type == 'display':
            return """### TASK 2: ADD DISPLAY-APPROPRIATE TESTS
- Content display tests
- Content formatting tests
- Viewer behavior tests (scroll, zoom if applicable)
- Close behavior tests
- State persistence tests

### DO NOT ADD:
- Input validation tests (viewers are read-only!)
- Numeric boundary tests (no numeric inputs!)"""

        elif feature_type == 'input':
            return """### TASK 2: ADD INPUT TESTS (boundary tests appropriate)
- Empty input behavior
- Minimum/maximum valid values
- Just outside valid range
- Special characters handling
- Long input handling
- Invalid input type"""

        elif feature_type == 'object_manipulation':
            return """### TASK 2: ADD OBJECT MANIPULATION TESTS
- Single object operations
- Undo/redo functionality
- State after operation
- Different object types
- Rapid repeated operations

### REMEMBER: No multi-object selection!"""

        else:
            return """### TASK 2: ADD RELEVANT EDGE CASES
Think about what makes sense for THIS specific feature.
Only add boundary/input tests if the feature has actual inputs."""

    def _get_feature_type_guidance(self) -> str:
        """Get guidance text for the detected feature type."""
        feature_type = self.ctx.feature_type

        guidance = {
            'navigation': "Navigation/menu features need: visibility, accessibility, keyboard tests. NO input/boundary tests!",
            'display': "Display/viewer features need: content display, formatting, viewer behavior tests. NO input validation tests!",
            'input': "Input features need: validation, boundary values, error handling tests. Boundary tests ARE appropriate here.",
            'calculation': "Calculation features need: accuracy, precision, edge values tests.",
            'object_manipulation': "Object manipulation needs: single-object ops, undo/redo, state tests. NO multi-select!",
            'general': "Generate tests relevant to the actual feature. Don't blindly add all test types!"
        }

        return guidance.get(feature_type, guidance['general'])

    @staticmethod
    def _format_list(items: List[str], prefix: str = "- ") -> str:
        """Format a list of items as bullet points."""
        if not items:
            return ""
        return "\n".join([f"{prefix}{item}" for item in items])


def build_prompts_for_project(
    config,  # ProjectConfig
    story_id: str,
    feature_name: str,
    acceptance_criteria: List[str],
    qa_prep: str,
    test_cases_json: str
) -> tuple:
    """
    Convenience function to build both system and user prompts.

    Returns:
        Tuple of (system_prompt, user_prompt)
    """
    builder = PromptBuilder.from_project_config(
        config=config,
        story_id=story_id,
        feature_name=feature_name,
        acceptance_criteria=acceptance_criteria,
        qa_prep=qa_prep
    )

    return builder.build_system_prompt(), builder.build_user_prompt(test_cases_json)
